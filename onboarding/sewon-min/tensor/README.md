# 텐서 개념
> multi-dimensional array, 파이토치에서 데이터에를 표현하는 기본단위. GPU에서 실행할 수 있다는 점이 ndarray와의 차이점

- AutoGrad (경사하강법 계산)에 최적화
- 텐서의 속성: shape, Rank, dtype, device => tensor.device는 GPU 프로그래밍할 때 확인하기 좋을 것 같다
- 딥러닝에서 모든 데이터는 텐서로 표현됨.
    - 입력 이미지: (채널, 높이, 너비)
    - 단어 임베딩: (문장 길이, 차원)
- 모델의 weight, bias도 모두 텐서
- 텐서를 통해 벡터, 행렬 연산을 수행할 수 있고,
- 파이토치는 텐서를 자동으로 GPU에 올리고 병렬 처리를 가능하게 함.

# `...`은 뭘까

```
tensor = torch.ones(4, 4)
print(f"First row: {tensor[0]}")
print(f"First column: {tensor[:, 0]}")
print(f"Last column: {tensor[..., -1]}")
tensor[:,1] = 0
print(tensor)
```
output
```
First row: tensor([1., 1., 1., 1.])
First column: tensor([1., 1., 1., 1.])
Last column: tensor([1., 1., 1., 1.])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
```


파이토치에서 `...` (ellipsis, 줄여서 `dot dot dot`)은 **전체 차원을 의미하는 와일드카드**로, **차원 수가 많을 때 유용하게 쓰는 문법**

---

## ✅ `...` (Ellipsis)의 의미

```python
tensor[..., -1]
```

> "마지막 차원의 마지막 원소를 가져오되, 앞의 차원들은 그대로 유지해줘."

---

## 🎯 구체 예시: `tensor[:, -1]` vs `tensor[..., -1]`

### 2차원 텐서에서는

```python
tensor[:, -1]         # (4, 4)에서 마지막 열 → shape: (4,)
tensor[..., -1]       # 동일한 의미 → shape: (4,)
```

### 3차원 이상일 때 차이 발생

```python
x = torch.randn(2, 3, 4)  # shape: (2, 3, 4)

x[:, :, -1]       # 마지막 차원만 -1로 인덱싱
x[..., -1]        # 동일, 더 일반적인 표현

# 둘 다 결과 shape: (2, 3)
```

---

## 🔧 `...`을 사용하는 이유

* 코드 **가독성** ↑
* 차원 수가 많아질 때 `:`를 일일이 쓰지 않아도 돼서 **코드 간결성** ↑
* 텐서 차원이 바뀌어도 **유연하게 대응** 가능

---

## 💡 정리

| 표현                | 의미                        |
| ----------------- | ------------------------- |
| `tensor[:, -1]`   | 2차원 텐서에서 마지막 열            |
| `tensor[..., -1]` | 어떤 차원이든, 마지막 차원에서 마지막 원소  |
| `tensor[..., 0]`  | 어떤 차원이든, 마지막 차원에서 첫 번째 원소 |

---

=> 결국 :, :, :, 이렇게 계속 쓰기 싫어서 간소화한 문법 같은데 차원이 굉장히 클 때 유용할 것 같다.


# torch.cat과 torch.stack의 차이

## 🆚 `torch.cat` vs `torch.stack`

| 항목     | `torch.cat`                  | `torch.stack`                   |
| ------ | ---------------------------- | ------------------------------- |
| ✅ 목적   | 텐서 **연결** (concatenate)      | 텐서 **쌓기** (stack)               |
| ✅ 차원 수 | 기존 차원 유지                     | 차원 수가 **1 증가**                  |
| ✅ 예시   | `[A, B] → (2, 3)` → `(4, 3)` | `[A, B] → (2, 3)` → `(2, 2, 3)` |
| ✅ 조건   | 연결할 차원을 제외하고 **shape 동일**    | 모든 텐서의 **shape 동일**             |

---

## 예제

```python
A = torch.tensor([[1, 2], [3, 4]])  # (2, 2)
B = torch.tensor([[5, 6], [7, 8]])  # (2, 2)

torch.cat([A, B], dim=0)  # (4, 2)
torch.stack([A, B], dim=0)  # (2, 2, 2)
```


## 근데 아래로 concat 하고 싶으면?(행 기준 연결)
텐서를 **아래로 이어붙이기**(= 행 기준 연결)하려면 `torch.cat(..., dim=0)`을 사용

---

## ✅ 아래로 이어붙이기 (행 기준 Concat)

```python
import torch

A = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)
B = torch.tensor([[5, 6], [7, 8]])  # shape: (2, 2)

C = torch.cat([A, B], dim=0)       # shape: (4, 2)

print(C)
```

### 결과:

```
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
```

---

## 🔁 비교: `dim=1`이면?

```python
C = torch.cat([A, B], dim=1)  # shape: (2, 4)

# 결과:
# tensor([[1, 2, 5, 6],
#         [3, 4, 7, 8]])
```

---

## 💡 요약

* **아래로 붙이기 (행 추가)**: `dim=0`
* **옆으로 붙이기 (열 추가)**: `dim=1`

둘 다 붙이기 전에, **붙이지 않는 차원들의 크기는 같아야** 함.
예: `dim=0`일 경우, 열 개수가 같아야 하고
`dim=1`일 경우, 행 개수가 같아야 해.


# 행렬 곱 VS 요소별 곱
사실 이 문법들은 **동일한 연산을 다른 방식으로 표현**한 거고, 때론 **in-place 방식**이나 **성능 최적화**를 위해 씀.

---

## ✅ 행렬 곱 vs 요소별 곱

| 구분        | 설명                   | 예시              |
| --------- | -------------------- | --------------- |
| **행렬 곱**  | 선형대수에서 말하는 **행렬 곱셈** | `@`, `matmul()` |
| **요소별 곱** | 같은 위치의 원소끼리 곱하는 연산   | `*`, `mul()`    |

---

## 🎯 각각 문법 설명

### ① **행렬 곱** (`@`, `matmul`)

```python
y1 = tensor @ tensor.T                # 파이썬 문법 지원 (권장)
y2 = tensor.matmul(tensor.T)         # 명시적 함수 방식
torch.matmul(tensor, tensor.T, out=y3)  # 결과를 y3에 저장 (메모리 재사용)
```

✅ `out=`을 쓰면 **결과를 기존 텐서에 저장** (메모리 재할당 방지)
→ 성능에 민감한 코드에서 사용

---

### ② **요소별 곱** (`*`, `mul`)

```python
z1 = tensor * tensor                 # 파이썬 문법 (가독성 ↑)
z2 = tensor.mul(tensor)             # 명시적 함수
torch.mul(tensor, tensor, out=z3)   # 기존 z3에 결과 저장
```

---

## 📌 요약

| 연산 유형 | 파이썬 연산자 | 파이토치 함수형   | `out=` 버전                    |
| ----- | ------- | ---------- | ---------------------------- |
| 행렬 곱  | `@`     | `matmul()` | `torch.matmul(..., out=...)` |
| 요소 곱  | `*`     | `mul()`    | `torch.mul(..., out=...)`    |

---

## 🔧 팁

* 초보자에게는 `@`, `*` 같은 연산자 형태가 더 **직관적**
* 반복 루프나 GPU 연산 최적화할 땐 `out=` 형태가 더 **효율적**


# in-place 연산 사용주의
바꿔치기(in-place) 연산 연산 결과를 피연산자(operand)에 저장하는 연산을 바꿔치기 연산이라고 부르며, _ 접미사를 갖습니다. 예를 들어: x.copy_(y) 나 x.t_() 는 x 를 변경합니다.


[16]
```python
print(f"{tensor} \n")
tensor.add_(5)
print(tensor)

tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]]) 

tensor([[6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.]])
```
Note
바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다. 따라서, 사용을 권장하지 않습니다.

# 곱셈에서 out=도 같은 맥락에서 위험하려나
⚠️ 왜 in-place가 위험할까?
파이토치는 **계산 그래프(컴퓨테이션 그래프)**를 만들어서 나중에 .backward()로 미분함.

근데 in-place 연산을 쓰면 중간 결과값이 덮어쓰기돼서,
나중에 미분할 때 필요한 값을 못 찾게 됨.

```python
x = torch.tensor([2.], requires_grad=True)
y = x * 2
y.add_(1)           # in-place 연산 → 위험
y.backward()        # 🔥 오류 또는 잘못된 결과 가능
```

✅ 반면 out=은?

```python
x = torch.tensor([2.], requires_grad=True)
y = x * 2
z = torch.empty_like(y)
torch.add(y, 1, out=z)  # 결과를 z에 저장 (y는 그대로 유지됨)
z.backward()            # ✅ 안전함
```
→ out=은 기존 값 변경이 아닌, 결과만 다른 곳에 저장하므로 일반적으로 autograd에 안전

단, out=에 넘기는 텐서가 계산 그래프의 중간값이라면 마찬가지로 주의



# Bridge

## 🔁 PyTorch ↔ NumPy 변환 요약

### ✅ 1. **텐서 → NumPy**

```python
t = torch.ones(5)
n = t.numpy()
```

* **공유 메모리**: `t`를 바꾸면 `n`도 바뀜
* 예:

```python
# t = [1, 1, 1, 1, 1]
t.add_(1)
print(n)  # → [2. 2. 2. 2. 2.]
```

---

### ✅ 2. **NumPy → 텐서**

```python
n = np.ones(5)
t = torch.from_numpy(n)
```

* 역시 **공유 메모리**
* 예:

```python
np.add(n, 1, out=n)
print(t)  # → tensor([2., 2., 2., 2., 2.])
```

---

## 🚨 주의: 공유는 **CPU 텐서**에서만 가능

* `.numpy()`와 `from_numpy()`는 **CPU 상에서만** 메모리 공유함.
* GPU 텐서는 `.numpy()` 불가능 → 먼저 `.to('cpu')` 필요

> numpy는 GPU 못 올라와서 그런가?? => YES 

```python
t = torch.ones(5, device='cuda')
t.cpu().numpy()  # OK
```

---

## 🧠 요약 문장

> 텐서와 NumPy 배열은 서로 **변환 시 메모리를 공유**하므로,
> 한쪽을 바꾸면 **다른 쪽도 함께 바뀐다 (단, CPU에서만 가능)**
